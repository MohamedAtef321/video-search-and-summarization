export NGC_API_KEY=ollama_not_needed
export NVIDIA_API_KEY="ollama" # Dummy key â€” Ollama does not require an API key
export HF_TOKEN=ollama_not_needed # Not needed for Ollama models

#Use latest public image 2.4.1.
export VIA_IMAGE=nvcr.io/nvidia/blueprint/vss-engine:2.4.1

#Adjust ports if needed
export FRONTEND_PORT=9100
export BACKEND_PORT=8100

#Change default user and pass if needed 
export GRAPH_DB_USERNAME=neo4j
export GRAPH_DB_PASSWORD=password

#Update paths local paths to config files if needed 
export CA_RAG_CONFIG=./config.yaml
export GUARDRAILS_CONFIG=./guardrails

# VLM: qwen3-vl:8b served locally via Ollama (OpenAI-compatible API)
export VLM_MODEL_TO_USE=openai-compat
export VIA_VLM_ENDPOINT="http://host.docker.internal:11434/v1"
export VIA_VLM_API_KEY="ollama"
export VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME="qwen3-vl:8b"
# export VLM_MAX_MODEL_LEN=20480  # Adjust if needed

#Adjust misc configs if needed
export DISABLE_GUARDRAILS=false

export NVIDIA_VISIBLE_DEVICES=0 #For H100 Deployment
#export NVIDIA_VISIBLE_DEVICES=0,1,2 #For L40S Deployment

export ENABLE_AUDIO=false
export RIVA_ASR_SERVER_URI=parakeet-ctc-asr
export RIVA_ASR_GRPC_PORT=50051
export RIVA_ASR_HTTP_PORT=9000
export RIVA_ASR_SERVER_IS_NIM=true
export ENABLE_RIVA_SERVER_READINESS_CHECK=true

export DISABLE_CV_PIPELINE=true
export INSTALL_PROPRIETARY_CODECS=false # Set to true when enabling CV
